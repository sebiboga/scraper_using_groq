name: GroqCloud Multi-Step Crawl

on:
  workflow_dispatch:
    inputs:
      start_url:
        description: 'Starting URL domain'
        required: true
        type: string
        default: 'https://farmec.ro'
      model:
        description: 'Select the model to use'
        required: true
        type: choice
        options:
          - compound-beta
          - compound-beta-mini
          - llama-3.3-70b-versatile
        default: compound-beta

jobs:
  crawl_career_page:
    runs-on: ubuntu-latest
    outputs:
      career_page: ${{ steps.call_api.outputs.career_page }}
    steps:
      - name: Call GroqCloud API to find career page
        id: call_api
        env:
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
          START_URL: ${{ github.event.inputs.start_url }}
          MODEL: ${{ github.event.inputs.model }}
        run: |
          SYSTEM_PROMPT="act like a smart crawler better than apache nutch; the crawling will start from a starting url that user is providing; first just identify the career page and print it out. no explanation needed. print this out as JSON and key to be career_page and put also the company in JSON. check for links in footer.  look for career page link. this is the most important part. no explanation needed new"
          USER_PROMPT="Starting URL: $START_URL . Respond *only* with the JSON object, no explanations, no extra text."

          PAYLOAD=$(jq -n --arg model "$MODEL" --arg sys "$SYSTEM_PROMPT" --arg user "$USER_PROMPT" '{
            model: $model,
            messages: [
              {role: "system", content: $sys},
              {role: "user", content: $user}
            ]
          }')

          RESPONSE=$(curl -s -X POST https://api.groq.com/openai/v1/chat/completions \
            -H "Content-Type: application/json" \
            -H "Authorization: Bearer $GROQ_API_KEY" \
            -d "$PAYLOAD")

          CONTENT=$(echo "$RESPONSE" | jq -r '.choices[0].message.content')

          # Extract JSON block if needed (assuming clean JSON here)
          CAREER_PAGE=$(echo "$CONTENT" | jq -r '.career_page')

          echo "career_page=$CAREER_PAGE" >> $GITHUB_OUTPUT

  crawl_job_links:
    runs-on: ubuntu-latest
    needs: crawl_career_page
    steps:
      - name: Call GroqCloud API to crawl job links from career page
        id: call_job_links
        env:
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
          CAREER_PAGE: ${{ needs.crawl_career_page.outputs.career_page }}
          MODEL: ${{ github.event.inputs.model }}
        run: |
          SYSTEM_PROMPT="act like a smart crawler like apache nutch; print out all urls you find in this page. do a crawl level 1; do not print out duplicates. filter those that have higher probability to be a job description pages; output as a JSON file with key job_link and company"
          USER_PROMPT="Starting URL: $CAREER_PAGE . Respond *only* with the JSON object, no explanations, no extra text."

          PAYLOAD=$(jq -n --arg model "$MODEL" --arg sys "$SYSTEM_PROMPT" --arg user "$USER_PROMPT" '{
            model: $model,
            messages: [
              {role: "system", content: $sys},
              {role: "user", content: $user}
            ]
          }')

          RESPONSE=$(curl -s -X POST https://api.groq.com/openai/v1/chat/completions \
            -H "Content-Type: application/json" \
            -H "Authorization: Bearer $GROQ_API_KEY" \
            -d "$PAYLOAD")

          CONTENT=$(echo "$RESPONSE" | jq -r '.choices[0].message.content')

          echo "Extracted job links JSON:"
          echo "$CONTENT"

          # Optionally set output if you want to use it later
          echo "job_links=$CONTENT" >> $GITHUB_OUTPUT
